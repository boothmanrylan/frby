import os
import argparse
import functools
import itertools
import six
import tensorflow as tf
import numpy as np

from tensorflow.python.framework import device as pydev
from tensorflow.python.training import device_setter
from tensorflow.core.framework import node_def_pb2

parser = argparse.ArgumentParser()
parser.add_argument(
    '--batch-size',
    default=32,
    type=int,
    help='The batch size')
parser.add_argument(
    '--train-steps',
    default=100,
    type=int,
    help='The number of training steps')
parser.add_argument(
    '--eval-steps',
    default=100,
    type=int,
    help='The number of steps to run during evaluation')
parser.add_argument(
    '--train-pattern',
    default='/scratch/r/rhlozek/rylan/tfrecords/*tfrecords',
    type=str,
    help='Unix file pattern to TFRecords containing training data')
parser.add_argument(
    '--eval-pattern',
    default='/scratch/r/rhlozek/rylan/tfrecords/*test',
    type=str,
    help='Unix file pattern to TFRecords containing test data')
parser.add_argument(
    '--job-dir',
    default='/home/r/rhlozek/rylan/frby/estimator/job_dir/',
    type=str,
    help='Directory to store checkpoint and evaluation files')
parser.add_argument(
    '--num-gpus',
    default=8,
    type=int,
    help='The number of GPUs to use')
parser.add_argument(
    '--variable-strategy',
    choices=['GPU', 'CPU'],
    default='GPU',
    type=str,
    help='Which device should perform variable operations')
parser.add_argument(
    '--num-layers',
    type=int,
    default=1,
    help='How many residual layers are in the model')
parser.add_argument(
    '--momentum',
    type=float,
    default=0.9,
    help='Momentum for MomentumOptimizer')
parser.add_argument(
    '--weight-decay',
    type=float,
    default=2e-1,
    help='Weight decay for convolutions')
parser.add_argument(
    '--learning-rate',
    type=float,
    default=0.1,
    help='Initial learning rate')
parser.add_argument(
    '--data-format',
    type=str,
    default='channels_first',
    choices=['channels_first', 'channels_last'])
parser.add_argument(
    '--batch-norm-decay',
    type=float,
    default=0.997,
    help='Decay for batch norm.')
parser.add_argument(
    '--batch-norm-epsilon',
    type=float,
    default=1e-5,
    help='Epsilon for batch norm.')
parser.add_argument(
    '--num-intra-threads',
    type=int,
    default=0,
    help='System picks operate value when set to 0')
parser.add_argument(
    '--num-classes',
    default=3,
    type=int,
    help='The number of classes in the dataset')
parser.add_argument(
    '--shuffle-buffer',
    default=30000,
    type=int,
    help='The size of the buffer used to shuffle the dataset')


class Constants(object):
    """
    Namespace object that holds the default height and width of the waterfall
    plots generated by mkdata
    """
    height = 1024
    width = 1596


class Model():
    def __init__(self, num_layers, is_training, data_format, batch_norm_decay,
                 batch_norm_epsilon, num_classes):
        self.batch_norm_decay = batch_norm_decay
        self.batch_norm_epsilon = batch_norm_epsilon
        self.is_training = is_training

        assert data_format in ('channels_first', 'channels_last')
        self.data_format = data_format

        self.num_layers = num_layers
        self.num_classes = num_classes
        self.filters = [16, 16, 32, 64]
        self.strides = [1, 2, 2]

    def forward_pass(self, x, input_data_format='channels_first'):
        if self.data_format != input_data_format:
            if input_data_format == 'channels_last':
                x = tf.transpose(x, [0, 3, 1, 2])
            else:
                x = tf.transpose(x, [0, 2, 3, 1])

        x = x / 128 - 1
        x = self._conv(x, 3, 16, 1)
        x = self._batch_norm(x)
        x = tf.nn.relu(x)

        for i in range(len(self.filters) - 1):
            with tf.name_scope('stage'):
                for j in range(self.num_layers):
                    if j == 0:
                        x = self.hidden_layer(x, 3, self.filters[i],
                                self.filters[i + 1], self.strides[i])
                    else:
                        x = self.hidden_layer(x, 3, self.filters[i + 1],
                                self.filters[i + 1], 1)
        x = self._global_avg_pool(x)
        x = self._fully_connected(x, self.num_classes)

        return x

    def hidden_layer(self, x, kernel_size, in_filter, out_filter, stride):
        with tf.name_scope('hidden_layer') as name_scope:
            orig_x = x

            x = self._conv(x, kernel_size, out_filter, stride)
            x = self._batch_norm(x)
            x = tf.nn.relu(x)
            x = self._conv(x, kernel_size, out_filter, 1)
            x = self._batch_norm(x)

            if in_filter != out_filter:
                orig_x = self._avg_pool(orig_x, stride, stride)
                pad = (out_filter - in_filter) // 2
                if self.data_format == 'channels_first':
                    pad_pattern = [[0, 0], [pad, pad], [0, 0], [0, 0]]
                else:
                    pad_pattern = [[0, 0], [0, 0], [0, 0], [pad, pad]]
                orig_x = tf.pad(orig_x, pad_pattern)

            x = tf.nn.relu(tf.add(x, orig_x))

            tf.logging.info('data after unit %s: %s', name_scope,
                    x.get_shape())
            return x

    def _conv(self, x, kernel_size, filters, strides):
        padding = 'SAME'
        if strides > 1:
            pad = kernel_size - 1
            beg = pad // 2
            end = pad - beg
            if self.data_format == 'channels_first':
                pad_pattern = [[0, 0], [0, 0], [beg, end], [beg, end]]
            else:
                pad_pattern = [[0, 0], [beg, end], [beg, end], [0, 0]]
            x = tf.pad(x, pad_pattern)
            padding = 'VALID'
        return tf.layers.conv2d(inputs=x, kernel_size=kernel_size,
                filters=filters, strides=strides, padding=padding,
                use_bias=False, data_format=self.data_format)

    def _batch_norm(self, x):
        with tf.name_scope('batch_norm') as name_scope:
            if self.data_format == 'channels_first':
                data_format = 'NCHW'
            else:
                data_format = 'NHWC'
            x = tf.contrib.layers.batch_norm(x, decay=self.batch_norm_decay,
                                             center=True, scale=True,
                                             epsilon=self.batch_norm_epsilon,
                                             is_training=self.is_training,
                                             fused=True,
                                             data_format=data_format)
            msg = 'data after unit %s: %s', name_scope, x.get_shape()
            tf.logging.info(msg)
        return x

    def _fully_connected(self, x, out_dim):
        with tf.name_scope('fully_connected') as name_scope:
            x = tf.layers.dense(x, out_dim)
            msg = 'data after unit %s: %s', name_scope, x.get_shape()
            tf.logging.info(msg)
        return x

    def _avg_pool(self, x, pool_size, stride):
        with tf.name_scope('avg_pool') as name_scope:
            x = tf.layers.average_pooling2d(x, pool_size, stride, 'SAME',
                    data_format=self.data_format)
            msg = 'data after unit %s: %s', name_scope, x.get_shape()
            tf.logging.info(msg)
        return x

    def _global_avg_pool(self, x):
        with tf.name_scope('global_avg_pool') as name_scope:
            assert x.get_shape().ndims == 4
            if self.data_format == 'channels_first':
                x = tf.reduce_mean(x, [2, 3])
            else:
                x = tf.reduce_mean(x, [1, 2])
            msg = 'data after unit %s: %s', name_scope, x.get_shape()
            tf.logging.info(msg)
        return x


class Dataset(object):
    def __init__(self, pattern):
        self.pattern = pattern

    def parser(self, example):
        features = tf.parse_single_example(
            example,
            features={
                'data': tf.FixedLenFeature([], tf.string),
                'label': tf.FixedLenFeature([], tf.int64)
            })

        data = tf.decode_raw(features['data'], tf.float32)
        data.set_shape(Constants.height * Constants.width)
        shape = tf.stack([1, Constants.height, Constants.width])
        data = tf.reshape(data, shape )
        data = tf.cast(data, tf.float32)

        label = tf.cast(features['label'], tf.int64)

        return data, label

    def make_batch(self, batch_size, shuffle=True, shuffle_buffer=20000):
        records = tf.data.Dataset.list_files(self.pattern)
        dataset = tf.data.TFRecordDataset(records).repeat()

        if shuffle:
            dataset = dataset.shuffle(buffer_size=shuffle_buffer)

        dataset = dataset.map(self.parser, num_parallel_calls=batch_size)
        dataset = dataset.batch(batch_size)

        iterator = dataset.make_one_shot_iterator()
        data_batch, label_batch = iterator.get_next()

        return data_batch, label_batch


def local_device_setter(num_devices=1, ps_device_type='cpu',
                        worker_device='/cpu:0', ps_ops=None,
                        ps_strategy=None):
    if ps_ops == None:
        ps_ops = ['Variable', 'VariableV2', 'VarHandleOp']

    if ps_strategy is None:
        ps_strategy = device_setter._RoundRobinStrategy(num_devices)

    if not six.callable(ps_strategy):
        raise TypeError('ps_strategy must be callable')

    def _local_device_chooser(op):
        current_device = pydev.DeviceSpec.from_string(op.device or "")
        node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def
        if node_def.op in ps_ops:
            spec_string = '/{}:{}'.format(ps_device_type, ps_strategy(op))
            ps_device_spec = pydev.DeviceSpec.from_string(spec_string)
            ps_device_spec.merge_from(current_device)
            return ps_device_spec.to_string()
        else:
            worker_dev_spec = pydev.DeviceSpec.from_string(worker_device or "")
            worker_dev_spec.merge_from(current_device)
            return worker_dev_spec.to_string()

    return _local_device_chooser


def input_fn(pattern, num_shards, batch_size, shuffle=True, buffer=20000):
    with tf.device('/cpu:0'):
        dset = Dataset(pattern)
        data_batch, label_batch = dset.make_batch(batch_size, shuffle, buffer)

        data_batch = tf.unstack(data_batch, num=batch_size, axis=0)
        label_batch = tf.unstack(label_batch, num=batch_size, axis=0)

        label_shards = [[] for _ in range(num_shards)]
        data_shards = [[] for _ in range(num_shards)]

        for i in range(batch_size):
            data_shards[i % num_shards].append(data_batch[i])
            label_shards[i % num_shards].append(label_batch[i])

        data_shards = [tf.parallel_stack(x) for x in data_shards]
        label_shards = [tf.parallel_stack(x) for x in label_shards]

        return data_shards, label_shards


def model_fn(features, labels, mode, params, config):
    is_training = (mode == tf.estimator.ModeKeys.TRAIN)
    weight_decay = params.weight_decay
    momentum = params.momentum
    num_workers = config.num_worker_replicas or 1

    losses = []
    grads = []
    preds = []

    data_format = params.data_format
    if not data_format:
        if params.num_gpus == 0:
            data_format = 'channels_last'
        else:
            data_format = 'channels_first'

    if params.num_gpus == 0:
        num_devices = 1
        device_type = 'cpu'
    else:
        num_devices = params.num_gpus
        device_type = 'gpu'

    for i in range(num_devices):
        worker_device = '/{}:{}'.format(device_type, i)
        if params.variable_strategy == 'CPU':
            device = local_device_setter(worker_device=worker_device)
        elif params.variable_strategy == 'GPU':
            device = local_device_setter(
                ps_device_type='gpu',
                worker_device=worker_device,
                ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(
                    params.num_gpus,
                    tf.contrib.training.byte_size_load_fn
                    )
                )
        with tf.variable_scope('model', reuse=bool(i != 0)):
            with tf.name_scope('tower_%d' % i) as name_scope:
                with tf.device(device):
                    loss, grad, pred = tower_fn(is_training, weight_decay,
                                                features[i], labels[i],
                                                data_format,
                                                params.num_layers,
                                                params.batch_norm_decay,
                                                params.batch_norm_epsilon,
                                                params.num_classes)
                    losses.append(loss)
                    grads.append(grad)
                    preds.append(pred)
                    if i == 0:
                        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,
                                                       name_scope)
    average_grads = []
    with tf.name_scope('gradient_averaging'):
        all_grads = {}
        # itertools.chain treats a list of iterables as one sequence
        for grad, var in itertools.chain(*grads):
            if grad is not None:
                all_grads.setdefault(var, []).append(grad)
        for var, grad in all_grads.items():
            with tf.device(var.device):
                # Sum all grad tensors and divide by len(grad) elementwise
                avg_grad = tf.multiply(tf.add_n(grad), 1. / len(grad))
            average_grads.append((avg_grad, var))

    consolidation = '/gpu:0' if params.variable_strategy == 'GPU' else '/cpu:0'
    with tf.device(consolidation):
        loss = tf.reduce_mean(losses, name='loss')

        batches_per_epoch = 32 * 4 // (params.batch_size * num_workers)
        boundaries = list(batches_per_epoch * np.array([50, 100, 300]))
        staged_lr = list(params.learning_rate * np.array([1, 0.1, 0.01, 0.002]))
        learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),
                                                    boundaries,
                                                    staged_lr)

        log_tensors = {'learning_rate': learning_rate, 'loss': loss}
        logging_hook = tf.train.LoggingTensorHook(log_tensors, every_n_iter=1)

        train_hooks = [logging_hook]

        optim = tf.train.MomentumOptimizer(learning_rate, momentum)
        train_op = [optim.apply_gradients(
                        average_grads, global_step=tf.train.get_global_step()
                        )]
        train_op.extend(update_ops)
        train_op = tf.group(*train_op)

        predictions = {
            'classes':
                tf.concat([p['classes'] for p in preds], axis=0),
            'probabilities':
                tf.concat([p['probabilities'] for p in preds], axis=0)
            }

        labels = tf.concat(labels, axis=0)
        metrics = {
            'accuracy':
                tf.metrics.accuracy(labels, predictions['classes']),
            'precision': # true positives / (true positives + false positives)
                tf.metrics.precision(labels, predictions['classes']),
            'recall': # true positives / (true positives + false negatives)
                tf.metrics.recall(labels, predictions['classes']),
            'true_positives':
                tf.metrics.true_positives(labels, predictions['classes']),
            'true_negatives':
                tf.metrics.true_negatives(labels, predictions['classes']),
            'false_positives':
                tf.metrics.false_positives(labels, predictions['classes']),
            'false_negatives':
                tf.metrics.false_negatives(labels, predictions['classes'])
            }

        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,
                                          loss=loss, train_op=train_op,
                                          training_hooks=train_hooks,
                                          eval_metric_ops=metrics)


def tower_fn(is_training, weight_decay, features, labels, data_format,
             num_layers, batch_norm_decay, batch_norm_epsilon, num_classes):
    model = Model(num_layers, batch_norm_decay=batch_norm_decay,
                  batch_norm_epsilon=batch_norm_epsilon,
                  is_training=is_training, data_format=data_format,
                  num_classes=num_classes)

    logits = model.forward_pass(features, input_data_format='channels_first')

    pred = {'classes': tf.argmax(input=logits, axis=1),
            'probabilities': tf.nn.softmax(logits)}

    loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)

    loss = tf.reduce_mean(loss)

    params = tf.trainable_variables()
    loss += weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in params])

    grad = tf.gradients(loss, params)

    return loss, zip(grad, params), pred


def main():
    params = parser.parse_args()

    os.environ['TF_SYNC_FINISH'] = '0'
    os.environ['TF_ENABLE_WINOGRAD_NOFUSED'] = '1'

    sess_config = tf.ConfigProto(
        allow_soft_placement=True,
        intra_op_parallelism_threads=params.num_intra_threads,
        gpu_options=tf.GPUOptions(force_gpu_compatible=True)
        )

    config = tf.estimator.RunConfig(session_config=sess_config,
                                    model_dir=params.job_dir)

    train_input_fn = functools.partial(input_fn, params.train_pattern,
                                       num_shards=params.num_gpus,
                                       batch_size=params.batch_size,
                                       shuffle=True,
                                       buffer=params.shuffle_buffer)

    eval_input_fn = functools.partial(input_fn, params.eval_pattern,
                                      num_shards=params.num_gpus,
                                      batch_size=params.batch_size,
                                      shuffle=True,
                                      buffer=params.shuffle_buffer)

    clsf = tf.estimator.Estimator(model_fn=model_fn, params=params,
                                  config=config)

    clsf.train(input_fn=train_input_fn, steps=params.train_steps)

    result = clsf.evaluate(input_fn=eval_input_fn, steps=params.eval_steps)

    print(result)

if __name__ == "__main__":
    tf.logging.set_verbosity(tf.logging.INFO)
    main()
